<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Using different kernels · LeastSquaresSVM</title><link rel="canonical" href="https://edwinb-ai.github.io/LeastSquaresSVM/stable/example3/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">LeastSquaresSVM</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../example1/">Classification of the Wisconsin breast cancer dataset</a></li><li><a class="tocitem" href="../example2/">Regression on a synthetic dataset</a></li><li class="is-active"><a class="tocitem" href>Using different kernels</a></li><li><a class="tocitem" href="../example4/">Multiclass classification</a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Using different kernels</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Using different kernels</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Using-different-kernels"><a class="docs-heading-anchor" href="#Using-different-kernels">Using different kernels</a><a id="Using-different-kernels-1"></a><a class="docs-heading-anchor-permalink" href="#Using-different-kernels" title="Permalink"></a></h1><p>Starting with version v0.6, two new kernels can be chosen: a <strong>linear</strong> kernel and a <strong>polynomial</strong> kernel. In this document we will see how to handle choosing a different kernel, and we&#39;ll showcase an example.</p><p>First, we need to import all the necessary packages.</p><pre><code class="language-julia">using LeastSquaresSVM
using MLJ, MLJBase
using DataFrames, CSV
using CategoricalArrays, Random
using Plots
gr();
rng = MersenneTwister(812);
</code></pre><p>For this example, we will create a large classification problem. It is actually inspired from a similar classification problem from <a href="https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py"><code>scikit-learn</code></a>.</p><p>The idea is to have a very large number of features (5000), and a small number of instances. This has been <a href="https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf">reported</a> as as being a good use case or rule of thumb</p><blockquote><p>Whenever the number of features is <em>larger</em> than the number of instances, use a linear kernel.</p></blockquote><pre><code class="language-julia">X, y = MLJ.make_blobs(500, 2_000; centers=2, cluster_std=[1.5, 0.5]);
</code></pre><p>Of course, this is just to showcase the implementation within <code>LeastSquaresSVM</code>. There are actually better ways to handle this kind of problem, e.g. dimensionality-reduction algorithms.</p><p>The <code>make_blobs</code> function is very similar to that of <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs"><code>scikit-learn</code>s</a>. The idea is to create circles, or clusters, and to classify between them.</p><p>We&#39;ll use them to test our <em>linear</em> kernel.</p><p>We need to construct a <code>DataFrame</code> with the arrays created to better handle the data, as well as a better integration with <code>MLJ</code>.</p><pre><code class="language-julia">df = DataFrame(X);
df.y = y;
</code></pre><p>Recall that we need to change the primitive types of <code>Julia</code> to <code>scitypes</code>.</p><pre><code class="language-julia">dfnew = coerce(df, autotype(df));
</code></pre><p>We can then observe the first three columns, together with their new types. We&#39;ll just look at the first 8 features to avoid cluttering the space.</p><pre><code class="language-julia">first(dfnew[:, 1:8], 3) |&gt; pretty</code></pre><pre><code class="language-none">┌────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┐
│ x1         │ x2         │ x3         │ x4         │ x5         │ x6         │ x7         │ x8         │
│ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │
│ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │
├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤
│ -1.38381   │ -10.0134   │ -3.41697   │ -0.330675  │ -2.51671   │ -3.36892   │ 5.17333    │ 9.19215    │
│ 7.8381     │ -3.24597   │ -1.97893   │ -6.60269   │ -5.00902   │ 0.111031   │ -7.87598   │ 7.4602     │
│ 8.98682    │ -2.5873    │ -1.78164   │ -5.34829   │ -6.32319   │ 0.190421   │ -8.23561   │ 7.67024    │
└────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┘
</code></pre><p>We should also check out the basic statistics of the dataset. We&#39;ll only use a small subset as the data frame it quite large.</p><pre><code class="language-julia">describe(dfnew[1:20, 1:10], :mean, :std, :eltype)</code></pre><pre><code class="language-none">10×4 DataFrame
 Row │ variable  mean       std      eltype
     │ Symbol    Float64    Float64  DataType
─────┼────────────────────────────────────────
   1 │ x1         1.81163   5.90859  Float64
   2 │ x2        -6.40472   3.06965  Float64
   3 │ x3        -4.33766   2.51566  Float64
   4 │ x4        -4.35342   1.90329  Float64
   5 │ x5        -3.09062   2.26507  Float64
   6 │ x6        -2.276     2.61437  Float64
   7 │ x7        -0.521823  7.09574  Float64
   8 │ x8         8.30596   1.34516  Float64
   9 │ x9         2.51241   1.68655  Float64
  10 │ x10       -0.283946  7.60654  Float64</code></pre><p>Recall that we also need to standardize the dataset, we can see here that the mean is close to zero, but not quite, and we also need an unitary standard deviation.</p><p>Split the dataset into training and testing sets.</p><pre><code class="language-julia">y, X = unpack(dfnew, ==(:y), colname -&gt; true);
train, test = partition(eachindex(y), 0.75, shuffle=true, rng=rng);
</code></pre><p>In this document, we will use a <em>pipeline</em> to integrate both the standardization and the LS-SVM classifier into one step. This should make it easier to train and to also showcase the ease of use of the <code>@pipeline</code> macro from <code>MLJ.jl</code>.</p><p>First, we define our classifier and the hyperparameter range. The <code>self_tuning_model</code> variable will hold the model with the best performance.</p><p>For the case of a <em>linear</em> kernel, no hyperparameter is needed. Instead, the only hyperparameter that needs to be adjusted is the <span>$\sigma$</span> value that is intrinsic of the least-squares formulation. We will search for a good hyperparameter now.</p><p>We will use the <code>accuracy</code> as a metric. The accuracy is simply defined as</p><p class="math-container">\[\text{accuracy} = \frac{\text{number of correct predictions}}{\text{total number of predictions}}\]</p><p>Note that the accuracy is not always a good measure of classification, but it will do fine on this dataset.</p><pre><code class="language-julia">model = LSSVClassifier(kernel=:linear);
r1 = range(model, :σ, lower=1.0, upper=1000.0);
self_tuning_model = TunedModel(
    model=model,
    tuning=Grid(goal=400, rng=rng),
    resampling=StratifiedCV(nfolds=5),
    range=[r1],
    measure=accuracy,
    acceleration=CPUThreads(), # We use this to enable multithreading
);
</code></pre><p>Then, we will build the pipeline. The first step is to standardize the inputs and then pass it to the classifier.</p><pre><code class="language-julia">pipe = @pipeline(Standardizer(), self_tuning_model);
</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Remember that the least-squares formulation uses <strong>all</strong> the data samples, so the following will actually consume at least 6 GB of RAM or more. Do not run this on your hardware if you are not sure you have this kind of resources available. At the very least, replace <code>CPUThreads()</code> with <code>CPU1()</code> to disable multithreading. Methods to handle memory more efficiently will be available in future versions.</p></div></div><p>And now we proceed to train all the models and find the best one!</p><pre><code class="language-julia">mach = machine(pipe, X, y);
fit!(mach, rows=train, verbosity=0);
fitted_params(mach).deterministic_tuned_model.best_model</code></pre><pre><code class="language-none">LSSVClassifier(
    kernel = :linear,
    γ = 1.0,
    σ = 283.9248120300752,
    degree = 0)[34m @564[39m</code></pre><p>Having found the best hyperparameters for the regressor model we proceed to check how the model generalizes. To do this we use the test set to check the performance.</p><pre><code class="language-julia">ŷ = MLJBase.predict(mach, rows=test);
result = accuracy(ŷ, y[test]);
result</code></pre><pre><code class="language-none">1.0</code></pre><p>We can see that we did quite well. A value of 1, or close enough, means the classifier is <em>perfect.</em> That is, it can classify correctly between each class.</p><p>Finally, let us look at the so-called <em>confusion matrix.</em> This table shows us useful information about the performance of our classifier.</p><p>Let us compute it first, and then we&#39;ll analyse it. Notice, however, that we need to first coerce the types to <code>OrderedFactor</code> <em>scitypes</em> in order for the confusion matrix to be computed correctly.</p><pre><code class="language-julia">ŷ = coerce(ŷ, OrderedFactor);
y_ordered = coerce(y[test], OrderedFactor);
confusion_matrix(ŷ, y_ordered)</code></pre><pre><code class="language-none">              ┌───────────────────────────┐
              │       Ground Truth        │
┌─────────────┼─────────────┬─────────────┤
│  Predicted  │      1      │      2      │
├─────────────┼─────────────┼─────────────┤
│      1      │     60      │      0      │
├─────────────┼─────────────┼─────────────┤
│      2      │      0      │     65      │
└─────────────┴─────────────┴─────────────┘
</code></pre><p>The way you read the confusion matrix is the following. The main diagonal tells us how many correct predictions were obtained by the classifier for both classes. On the other hand, the other values are the following</p><ul><li>The <em>upper right</em> value is known as the <strong>false positive</strong>. This is the number of instances that were classified as belonging to a given class, when actually they were instances of the other one. An example would be if we had an instance <span>$x_1$</span> which belonged to the class <code>b</code>, but the classifier would have predicted it actually belonged to class <code>a</code>.</li><li>The <em>lower left</em> value is known as the <strong>false negative</strong>. This is the number of instances classified as <em>not</em> belonging to a given class, when they actually belonged to a class. An example would be if we had an instance <span>$x_2$</span> which belonged to class <code>a</code>, and the classifier actually predicted it belonged to class <code>b</code>.</li></ul><p>It might be a little bit confusing, so a good starting point for more information on the subject is the excellent <a href="https://en.wikipedia.org/wiki/Confusion_matrix">Wikipedia article.</a> You might also be interested in the following <a href="https://developers.google.com/machine-learning/crash-course/classification/accuracy">document</a> from a Google&#39;s Machine Learning Crash Course.</p><p>In this case, we can see that no false negative or positive values were found, which means that the classifier did outstandingly good. Normally, we can expect to have at least some percentage of false negative or positives.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../example2/">« Regression on a synthetic dataset</a><a class="docs-footer-nextpage" href="../example4/">Multiclass classification »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 23 July 2021 22:18">Friday 23 July 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
