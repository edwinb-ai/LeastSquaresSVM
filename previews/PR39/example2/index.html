<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Regression on a synthetic dataset · LeastSquaresSVM</title><link rel="canonical" href="https://edwinb-ai.github.io/LeastSquaresSVM/stable/example2/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">LeastSquaresSVM</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../example1/">Classification of the Wisconsin breast cancer dataset</a></li><li class="is-active"><a class="tocitem" href>Regression on a synthetic dataset</a></li><li><a class="tocitem" href="../example3/">Using different kernels</a></li><li><a class="tocitem" href="../example4/">Multiclass classification</a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Regression on a synthetic dataset</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Regression on a synthetic dataset</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Regression-on-a-synthetic-dataset"><a class="docs-heading-anchor" href="#Regression-on-a-synthetic-dataset">Regression on a synthetic dataset</a><a id="Regression-on-a-synthetic-dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Regression-on-a-synthetic-dataset" title="Permalink"></a></h1><p>In this page we will see how to perform Least Squares Support Vector Regression using <code>LeastSquaresSVM</code>. To accomplish this task, we will use synthetic data as created by the <code>make_regression</code> function from <code>MLJ</code>.</p><p>First, we need to import all the necessary packages.</p><pre><code class="language-julia">using LeastSquaresSVM
using MLJ, MLJBase
using DataFrames, CSV
using CategoricalArrays, Random
using Plots
gr();
rng = MersenneTwister(88);</code></pre><p>We then create the regression problem. To really push the implementation we will create a problem with 5 features and 500 instances/observations. We will also add a little bit of Gaussian noise to the problem.</p><pre><code class="language-julia">X, y = MLJ.make_regression(500, 5; noise=1.0, rng=rng);</code></pre><p>We need to construct a <code>DataFrame</code> with the arrays created to better handle the data, as well as a better integration with <code>MLJ</code>.</p><pre><code class="language-julia">df = DataFrame(X);
df.y = y;</code></pre><p>A very important part of the <code>MLJ</code> framework is its use of <code>scitypes</code>, a special kind of type that work together with the objects from the framework. Because the regression problem has the <code>Julia</code> types we need to convert this types to correct <code>scitypes</code> such such that the <code>machine</code>s from <code>MLJ</code> work fine.</p><pre><code class="language-julia">dfnew = coerce(df, autotype(df));</code></pre><p>We can then observe the first three columns, together with their new types.</p><pre><code class="language-julia">first(dfnew, 3) |&gt; pretty</code></pre><pre class="documenter-example-output">┌────────────┬────────────┬────────────┬────────────┬────────────┬────────────┐
│ x1         │ x2         │ x3         │ x4         │ x5         │ y          │
│ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │
│ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │
├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤
│ 0.548308   │ -0.138211  │ -0.972058  │ 0.208522   │ -0.176455  │ -0.227663  │
│ -0.719038  │ -0.0480897 │ 2.04784    │ 0.171147   │ 0.307089   │ 4.35437    │
│ 0.96055    │ 0.306524   │ -0.445852  │ 0.611987   │ 1.3417     │ -4.34867   │
└────────────┴────────────┴────────────┴────────────┴────────────┴────────────┘</pre><p>We should also check out the basic statistics of the dataset.</p><pre><code class="language-julia">describe(dfnew, :mean, :std, :eltype)</code></pre><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>std</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Float64</th><th>DataType</th></tr></thead><tbody><p>6 rows × 4 columns</p><tr><th>1</th><td>x1</td><td>-0.0821964</td><td>1.06794</td><td>Float64</td></tr><tr><th>2</th><td>x2</td><td>-0.0304101</td><td>1.0045</td><td>Float64</td></tr><tr><th>3</th><td>x3</td><td>0.00888428</td><td>0.99942</td><td>Float64</td></tr><tr><th>4</th><td>x4</td><td>-0.053676</td><td>0.972936</td><td>Float64</td></tr><tr><th>5</th><td>x5</td><td>-0.0263042</td><td>1.00545</td><td>Float64</td></tr><tr><th>6</th><td>y</td><td>0.526471</td><td>3.12977</td><td>Float64</td></tr></tbody></table><p>Recall that we also need to standardize the dataset, we can see here that the mean is close to zero, but not quite, and we also need an unitary standard deviation.</p><p>Split the dataset into training and testing sets.</p><pre><code class="language-julia">y, X = unpack(dfnew, ==(:y), colname -&gt; true);
train, test = partition(eachindex(y), 0.75, shuffle=true, rng=rng);
stand1 = Standardizer();
X = MLJBase.transform(MLJBase.fit!(MLJBase.machine(stand1, X)), X);</code></pre><pre class="documenter-example-output">[ Info: Training [34mMachine{Standardizer} @163[39m.</pre><p>We should make sure that the features have mean close to zero and an unitary standard deviation.</p><pre><code class="language-julia">describe(X |&gt; DataFrame, :mean, :std, :eltype)</code></pre><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>std</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Float64</th><th>DataType</th></tr></thead><tbody><p>5 rows × 4 columns</p><tr><th>1</th><td>x1</td><td>3.73035e-17</td><td>1.0</td><td>Float64</td></tr><tr><th>2</th><td>x2</td><td>-2.75335e-17</td><td>1.0</td><td>Float64</td></tr><tr><th>3</th><td>x3</td><td>3.33067e-18</td><td>1.0</td><td>Float64</td></tr><tr><th>4</th><td>x4</td><td>1.24345e-17</td><td>1.0</td><td>Float64</td></tr><tr><th>5</th><td>x5</td><td>-2.26485e-17</td><td>1.0</td><td>Float64</td></tr></tbody></table><p>Define a good set of hyperparameters for this problem and train the regressor. We will use the amazing capability of <code>MLJ</code> to tune a model and return the best model found.</p><p>For this we have taken some judicious guessing on the best values that the hyperparameters should take. We employ 5-fold cross-validation and a 400 by 400 grid of points to do an exhaustive search.</p><p>We will train the regressor using the root mean square error which is defined as follows</p><p class="math-container">\[RMSE = \sqrt{\frac{\sum_{i=1}^{N} \left(\hat{y}_i - y_i \right)^2}{N}}\]</p><p>where we define <span>$\hat{y}_i$</span> as the <em>predicted value</em>, and <span>$y_i$</span> as the real value.</p><pre><code class="language-julia">model = LeastSquaresSVM.LSSVRegressor();
r1 = MLJBase.range(model, :σ, lower=7e-4, upper=1e-3);
r2 = MLJBase.range(model, :γ, lower=120.0, upper=130.0);
self_tuning_model = TunedModel(
    model=model,
    tuning=Grid(goal=400, rng=rng),
    resampling=CV(nfolds=5),
    range=[r1, r2],
    measure=MLJBase.rms,
    acceleration=CPUThreads(), # We use this to enable multithreading
);</code></pre><p>And now we proceed to train all the models and find the best one!</p><pre><code class="language-">mach = MLJ.machine(self_tuning_model, X, y);
MLJBase.fit!(mach, rows=train, verbosity=0);
fitted_params(mach).best_model</code></pre><p>Having found the best hyperparameters for the regressor model we proceed to check how the model generalizes and we use the test set to check the performance.</p><pre><code class="language-">ŷ = MLJBase.predict(mach, rows=test);
result = round(MLJBase.rms(ŷ, y[test]), sigdigits=4);
@show result # Check the result</code></pre><p>We can see that we did quite well. A value of 1, or close enough, is good. We expect it to reach a lower value, closer to zero, but maybe we needed more refinement in the grid search.</p><p>We can also see a plot of the predicted and true values. The closer these dots are to the diagonal means that the model performed well.</p><pre><code class="language-">scatter(ŷ, y[test], markersize=9)
r = range(minimum(y[test]), maximum(y[test]); length=length(test))
plot!(r, r, linewidth=9)
plot!(size=(3000, 2100))</code></pre><p>We can actually see that we are not that far off, maybe a little more search could definitely improve the performance of our model.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../example1/">« Classification of the Wisconsin breast cancer dataset</a><a class="docs-footer-nextpage" href="../example3/">Using different kernels »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 27 September 2021 01:47">Monday 27 September 2021</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
