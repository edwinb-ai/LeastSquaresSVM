var documenterSearchIndex = {"docs":
[{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"EditURL = \"<unknown>/src/examples/example4.jl\"","category":"page"},{"location":"example4/#Multiclass-classification","page":"Multiclass classification","title":"Multiclass classification","text":"","category":"section"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"As of version v0.7 and over, the LSSVClassifier can handle multiclass classification problems. It does so by using the so-called one-vs-one approach. This is described as follows:","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"For a classification problem with k classes, one trains k(k-1)2 binary classifiers.\nEach binary classifier deals with every possible class pair combination. For instance, for a three class classification problem, we need to train three classifiers. The first will deal with the (1, 2) binary classification problem. The second with the (1, 3), while the third classifier will deal with the (2, 3) class pair.\nWe then save all these parameters, and on the prediction step, we ask every classifier to predict a given data instance. Care must be taken to ensure that the class pairs are decoded correctly.\nA voting scheme is then applied to these predictions. The class that has the largest number of votes when a pediction is carried out is the class that is taken as the predicted class. In the important case of a tie, the predicted class is always that of the lowest index. This is obviously not a great strategy, but it's simple enough to break the tie. See[1] to read more about this strategy.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"Here, we deal with the binary classification problems with the underlying implementation of the LSSVClassifier and finally pool all of our results together. We then apply the voting scheme and we are done.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"In this example we will showcase this ability on the very famous and not so difficult problem of the Iris flower dataset.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"This problem is a 3-class classification problem, with 4 features and a total of 150 samples; 50 for each class. It is expected that with a good set of hyperparameters, our implementation should easily obtain a 100% accuracy classification rate on this problem.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"Let us begin by importing our packages and setting an RNG to ensure reproducibility.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"using MLJ, MLJBase\nusing Random\nusing LeastSquaresSVM\n\nrng = MersenneTwister(957);\n","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"The Iris dataset is so famous, it is included in the MLJ.jl package. By using the very convenient macro @load_iris we can obtain the full dataset, as follows.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"X, y = @load_iris;\n","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"Then, we split the dataset. We have said before that the datset is small, so we will keep this in mind. A 60-40 train-test split should suffice.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"train, test = partition(eachindex(y), 0.6, shuffle=true, rng=rng);\n","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"We will now instantiate our model. Recall that the default kernel is the RBF kernel.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"model = LSSVClassifier();\n","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"Now we set some ranges for the hyperparameters and also instantiate a self-tuning model Note however that a repeats option is now used. What this does is to repeat the resampling of the dataset that many times. This is just to ensure that the model is learning correctly and not being biased.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"r1 = range(model, :σ, lower=1e-2, upper=1e-1);\nr2 = range(model, :γ, lower=140, upper=150);\nself_tuning_model = TunedModel(\n    model=model,\n    tuning=Grid(goal=200, rng=rng),\n    resampling=StratifiedCV(nfolds=5),\n    range=[r1, r2],\n    measure=accuracy,\n    acceleration=CPUThreads(),\n    repeats=10 # Add more resampling to be sure we are not biased\n);\n","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"We will use a pipeline to first standardize the data and then feed it to the model.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"pipe = @pipeline(Standardizer(), self_tuning_model);\nmach = MLJ.machine(pipe, X, y);\n","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"It's time to train. Remeber that if you are going to use multithreading for the hyperparameter search, you need to start your Julia session with the environment variable JULIA_NUM_THREADS set to a different number than 1.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"MLJ.fit!(mach, rows=train, verbosity=0);\n","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"We want to see the best set of hyperparameters.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"fitted_params(mach).deterministic_tuned_model.best_model","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"LSSVClassifier(\n    kernel = :rbf,\n    γ = 147.69230769230768,\n    σ = 0.01,\n    degree = 0)\u001b[34m @315\u001b[39m","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"Great! Let us see how good was the model. Let's check with the test set.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"results = MLJ.predict(mach, rows=test);\nacc = MLJ.accuracy(results, y[test]);\nacc","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"1.0","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"Fantastic! A 100% accuracy, as was expected. As a good measure, let us inspect the confusion matrix of the classification problem.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"results = coerce(results, OrderedFactor);\ny_ordered = coerce(y[test], OrderedFactor);\nconfusion_matrix(results, y_ordered)","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"              ┌─────────────────────────────────────────┐\n              │              Ground Truth               │\n┌─────────────┼─────────────┬─────────────┬─────────────┤\n│  Predicted  │   setosa    │  versicol…  │  virginica  │\n├─────────────┼─────────────┼─────────────┼─────────────┤\n│   setosa    │     19      │      0      │      0      │\n├─────────────┼─────────────┼─────────────┼─────────────┤\n│  versicol…  │      0      │     21      │      0      │\n├─────────────┼─────────────┼─────────────┼─────────────┤\n│  virginica  │      0      │      0      │     20      │\n└─────────────┴─────────────┴─────────────┴─────────────┘\n","category":"page"},{"location":"example4/#Conclusions","page":"Multiclass classification","title":"Conclusions","text":"","category":"section"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"Again, just as expected, the result is perfect. But do note that this does not mean that the implementation or the model is the best there is. There are other models and implementations that should be checked along with this one.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"As a good rule of thumb, when doing machine learning, always try to train at least one more model to compare performance.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"[1]: Chih-Wei Hsu and Chih-Jen Lin (2002) ‘A comparison of methods for multiclass support vector machines’, IEEE Transactions on Neural Networks, 13(2), pp. 416. doi: 10.1109/72.991427.","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"","category":"page"},{"location":"example4/","page":"Multiclass classification","title":"Multiclass classification","text":"This page was generated using Literate.jl.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"EditURL = \"<unknown>/src/examples/example2.jl\"","category":"page"},{"location":"example2/#Regression-on-a-synthetic-dataset","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"","category":"section"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"In this page we will see how to perform Least Squares Support Vector Regression using LeastSquaresSVM. To accomplish this task, we will use synthetic data as created by the make_regression function from MLJ.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"First, we need to import all the necessary packages.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"using LeastSquaresSVM\nusing MLJ, MLJBase\nusing DataFrames, CSV\nusing CategoricalArrays, Random\nusing Plots\ngr();\nrng = MersenneTwister(88);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We then create the regression problem. To really push the implementation we will create a problem with 5 features and 500 instances/observations. We will also add a little bit of Gaussian noise to the problem.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"X, y = MLJ.make_regression(500, 5; noise=1.0, rng=rng);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We need to construct a DataFrame with the arrays created to better handle the data, as well as a better integration with MLJ.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"df = DataFrame(X);\ndf.y = y;\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"A very important part of the MLJ framework is its use of scitypes, a special kind of type that work together with the objects from the framework. Because the regression problem has the Julia types we need to convert this types to correct scitypes such such that the machines from MLJ work fine.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"dfnew = coerce(df, autotype(df));\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can then observe the first three columns, together with their new types.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"first(dfnew, 3) |> pretty","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We should also check out the basic statistics of the dataset.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"describe(dfnew, :mean, :std, :eltype)","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Recall that we also need to standardize the dataset, we can see here that the mean is close to zero, but not quite, and we also need an unitary standard deviation.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Split the dataset into training and testing sets.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"y, X = unpack(dfnew, ==(:y), colname -> true);\ntrain, test = partition(eachindex(y), 0.75, shuffle=true, rng=rng);\nstand1 = Standardizer();\nX = MLJBase.transform(MLJBase.fit!(MLJBase.machine(stand1, X)), X);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We should make sure that the features have mean close to zero and an unitary standard deviation.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"describe(X |> DataFrame, :mean, :std, :eltype)","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Define a good set of hyperparameters for this problem and train the regressor. We will use the amazing capability of MLJ to tune a model and return the best model found.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"For this we have taken some judicious guessing on the best values that the hyperparameters should take. We employ 5-fold cross-validation and a 400 by 400 grid of points to do an exhaustive search.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We will train the regressor using the root mean square error which is defined as follows","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"RMSE = sqrtfracsum_i=1^N left(haty_i - y_i right)^2N","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"where we define haty_i as the predicted value, and y_i as the real value.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"model = LeastSquaresSVM.LSSVRegressor();\nr1 = MLJBase.range(model, :σ, lower=7e-4, upper=1e-3);\nr2 = MLJBase.range(model, :γ, lower=120.0, upper=130.0);\nself_tuning_model = TunedModel(\n    model=model,\n    tuning=Grid(goal=400, rng=rng),\n    resampling=CV(nfolds=5),\n    range=[r1, r2],\n    measure=MLJBase.rms,\n    acceleration=CPUThreads(), # We use this to enable multithreading\n);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"And now we proceed to train all the models and find the best one!","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"mach = MLJ.machine(self_tuning_model, X, y);\nMLJBase.fit!(mach, rows=train, verbosity=0);\nfitted_params(mach).best_model","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Having found the best hyperparameters for the regressor model we proceed to check how the model generalizes and we use the test set to check the performance.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"ŷ = MLJBase.predict(mach, rows=test);\nresult = round(MLJBase.rms(ŷ, y[test]), sigdigits=4);\n@show result # Check the result","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can see that we did quite well. A value of 1, or close enough, is good. We expect it to reach a lower value, closer to zero, but maybe we needed more refinement in the grid search.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can also see a plot of the predicted and true values. The closer these dots are to the diagonal means that the model performed well.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"scatter(ŷ, y[test], markersize=9)\nr = range(minimum(y[test]), maximum(y[test]); length=length(test))\nplot!(r, r, linewidth=9)\nplot!(size=(3000, 2100))","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can actually see that we are not that far off, maybe a little more search could definitely improve the performance of our model.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"This page was generated using Literate.jl.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"EditURL = \"<unknown>/src/examples/example3.jl\"","category":"page"},{"location":"example3/#Using-different-kernels","page":"Using different kernels","title":"Using different kernels","text":"","category":"section"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Starting with version v0.6, two new kernels can be chosen: a linear kernel and a polynomial kernel. In this document we will see how to handle choosing a different kernel, and we'll showcase an example.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"First, we need to import all the necessary packages.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"using LeastSquaresSVM\nusing MLJ, MLJBase\nusing DataFrames, CSV\nusing CategoricalArrays, Random\nusing Plots\ngr();\nrng = MersenneTwister(812);\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"For this example, we will create a large classification problem. It is actually inspired from a similar classification problem from scikit-learn.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"The idea is to have a very large number of features (5000), and a small number of instances. This has been reported as as being a good use case or rule of thumb","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Whenever the number of features is larger than the number of instances, use a linear kernel.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"X, y = MLJ.make_blobs(500, 2_000; centers=2, cluster_std=[1.5, 0.5]);\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Of course, this is just to showcase the implementation within LeastSquaresSVM. There are actually better ways to handle this kind of problem, e.g. dimensionality-reduction algorithms.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"The make_blobs function is very similar to that of scikit-learns. The idea is to create circles, or clusters, and to classify between them.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We'll use them to test our linear kernel.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We need to construct a DataFrame with the arrays created to better handle the data, as well as a better integration with MLJ.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"df = DataFrame(X);\ndf.y = y;\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Recall that we need to change the primitive types of Julia to scitypes.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"dfnew = coerce(df, autotype(df));\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We can then observe the first three columns, together with their new types. We'll just look at the first 8 features to avoid cluttering the space.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"first(dfnew[:, 1:8], 3) |> pretty","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"┌────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┐\n│ x1         │ x2         │ x3         │ x4         │ x5         │ x6         │ x7         │ x8         │\n│ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │\n│ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │\n├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n│ -1.38381   │ -10.0134   │ -3.41697   │ -0.330675  │ -2.51671   │ -3.36892   │ 5.17333    │ 9.19215    │\n│ 7.8381     │ -3.24597   │ -1.97893   │ -6.60269   │ -5.00902   │ 0.111031   │ -7.87598   │ 7.4602     │\n│ 8.98682    │ -2.5873    │ -1.78164   │ -5.34829   │ -6.32319   │ 0.190421   │ -8.23561   │ 7.67024    │\n└────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┘\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We should also check out the basic statistics of the dataset. We'll only use a small subset as the data frame it quite large.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"describe(dfnew[1:20, 1:10], :mean, :std, :eltype)","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"10×4 DataFrame\n Row │ variable  mean       std      eltype\n     │ Symbol    Float64    Float64  DataType\n─────┼────────────────────────────────────────\n   1 │ x1         1.81163   5.90859  Float64\n   2 │ x2        -6.40472   3.06965  Float64\n   3 │ x3        -4.33766   2.51566  Float64\n   4 │ x4        -4.35342   1.90329  Float64\n   5 │ x5        -3.09062   2.26507  Float64\n   6 │ x6        -2.276     2.61437  Float64\n   7 │ x7        -0.521823  7.09574  Float64\n   8 │ x8         8.30596   1.34516  Float64\n   9 │ x9         2.51241   1.68655  Float64\n  10 │ x10       -0.283946  7.60654  Float64","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Recall that we also need to standardize the dataset, we can see here that the mean is close to zero, but not quite, and we also need an unitary standard deviation.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Split the dataset into training and testing sets.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"y, X = unpack(dfnew, ==(:y), colname -> true);\ntrain, test = partition(eachindex(y), 0.75, shuffle=true, rng=rng);\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"In this document, we will use a pipeline to integrate both the standardization and the LS-SVM classifier into one step. This should make it easier to train and to also showcase the ease of use of the @pipeline macro from MLJ.jl.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"First, we define our classifier and the hyperparameter range. The self_tuning_model variable will hold the model with the best performance.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"For the case of a linear kernel, no hyperparameter is needed. Instead, the only hyperparameter that needs to be adjusted is the sigma value that is intrinsic of the least-squares formulation. We will search for a good hyperparameter now.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We will use the accuracy as a metric. The accuracy is simply defined as","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"textaccuracy = fractextnumber of correct predictionstexttotal number of predictions","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Note that the accuracy is not always a good measure of classification, but it will do fine on this dataset.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"model = LSSVClassifier(kernel=:linear);\nr1 = range(model, :σ, lower=1.0, upper=1000.0);\nself_tuning_model = TunedModel(\n    model=model,\n    tuning=Grid(goal=400, rng=rng),\n    resampling=StratifiedCV(nfolds=5),\n    range=[r1],\n    measure=accuracy,\n    acceleration=CPUThreads(), # We use this to enable multithreading\n);\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Then, we will build the pipeline. The first step is to standardize the inputs and then pass it to the classifier.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"pipe = @pipeline(Standardizer(), self_tuning_model);\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"warning: Warning\nRemember that the least-squares formulation uses all the data samples, so the following will actually consume at least 6 GB of RAM or more. Do not run this on your hardware if you are not sure you have this kind of resources available. At the very least, replace CPUThreads() with CPU1() to disable multithreading. Methods to handle memory more efficiently will be available in future versions.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"And now we proceed to train all the models and find the best one!","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"mach = machine(pipe, X, y);\nfit!(mach, rows=train, verbosity=0);\nfitted_params(mach).deterministic_tuned_model.best_model","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"LSSVClassifier(\n    kernel = :linear,\n    γ = 1.0,\n    σ = 283.9248120300752,\n    degree = 0)\u001b[34m @564\u001b[39m","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Having found the best hyperparameters for the regressor model we proceed to check how the model generalizes. To do this we use the test set to check the performance.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"ŷ = MLJBase.predict(mach, rows=test);\nresult = accuracy(ŷ, y[test]);\nresult","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"1.0","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We can see that we did quite well. A value of 1, or close enough, means the classifier is perfect. That is, it can classify correctly between each class.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Finally, let us look at the so-called confusion matrix. This table shows us useful information about the performance of our classifier.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Let us compute it first, and then we'll analyse it. Notice, however, that we need to first coerce the types to OrderedFactor scitypes in order for the confusion matrix to be computed correctly.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"ŷ = coerce(ŷ, OrderedFactor);\ny_ordered = coerce(y[test], OrderedFactor);\nconfusion_matrix(ŷ, y_ordered)","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"              ┌───────────────────────────┐\n              │       Ground Truth        │\n┌─────────────┼─────────────┬─────────────┤\n│  Predicted  │      1      │      2      │\n├─────────────┼─────────────┼─────────────┤\n│      1      │     60      │      0      │\n├─────────────┼─────────────┼─────────────┤\n│      2      │      0      │     65      │\n└─────────────┴─────────────┴─────────────┘\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"The way you read the confusion matrix is the following. The main diagonal tells us how many correct predictions were obtained by the classifier for both classes. On the other hand, the other values are the following","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"The upper right value is known as the false positive. This is the number of instances that were classified as belonging to a given class, when actually they were instances of the other one. An example would be if we had an instance x_1 which belonged to the class b, but the classifier would have predicted it actually belonged to class a.\nThe lower left value is known as the false negative. This is the number of instances classified as not belonging to a given class, when they actually belonged to a class. An example would be if we had an instance x_2 which belonged to class a, and the classifier actually predicted it belonged to class b.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"It might be a little bit confusing, so a good starting point for more information on the subject is the excellent Wikipedia article. You might also be interested in the following document from a Google's Machine Learning Crash Course.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"In this case, we can see that no false negative or positive values were found, which means that the classifier did outstandingly good. Normally, we can expect to have at least some percentage of false negative or positives.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"This page was generated using Literate.jl.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"EditURL = \"<unknown>/src/examples/example1.jl\"","category":"page"},{"location":"example1/#Classification-of-the-Wisconsin-breast-cancer-dataset","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"","category":"section"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"In this case study we will deal with the Wisconsin breast cancer dataset which can be browsed freely on the UCI website.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"In particular, this dataset contains 10 features and 699 instances. In the work we will do here, however, we will skip some instances due to some missing values.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"The dataset contains only two classes, and the purpose is to use all ten features to answer a simple question:","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Does the subject have a benign or malign tumor?","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"To answer this question, we will train a Least Squares Support Vector Machine as implemented in LeastSquaresSVM.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"First, we need to import all the necessary packages.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"using MLJ, MLJBase\nusing DataFrames, CSV\nusing CategoricalArrays\nusing Random, Statistics\nusing LeastSquaresSVM","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We then need to specify a seed to enable reproducibility of the results.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"rng = MersenneTwister(801239);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Here we are creating a list with all the headers.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"headers = [\n\t\"id\", \"Clump Thickness\",\n\t\"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\n\t\"Marginal Adhesion\", \"Single Epithelial Cell Size\",\n\t\"Bare Nuclei\", \"Bland Chromatin\",\n\t\"Normal Nucleoli\", \"Mitoses\", \"class\"\n];\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We define the path were the dataset is located","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"path = joinpath(\"src\", \"examples\", \"wbc.csv\");\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We load the csv file and convert it to a DataFrame. Note that we are specifying to the file reader to replace the string ? to a missing value. This dataset contains the the string ? when there is a value missing.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"data = CSV.File(path; header=headers, missingstring=\"?\") |> DataFrame;\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We can display the first 10 rows from the dataset","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"first(data, 10)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"10×11 DataFrame\n Row │ id       Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  class\n     │ Int64    Int64            Int64                    Int64                     Int64              Int64                        Int64?       Int64            Int64            Int64    Int64\n─────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ 1000025                5                        1                         1                  1                            2            1                3                1        1      2\n   2 │ 1002945                5                        4                         4                  5                            7           10                3                2        1      2\n   3 │ 1015425                3                        1                         1                  1                            2            2                3                1        1      2\n   4 │ 1016277                6                        8                         8                  1                            3            4                3                7        1      2\n   5 │ 1017023                4                        1                         1                  3                            2            1                3                1        1      2\n   6 │ 1017122                8                       10                        10                  8                            7           10                9                7        1      4\n   7 │ 1018099                1                        1                         1                  1                            2           10                3                1        1      2\n   8 │ 1018561                2                        1                         2                  1                            2            1                3                1        1      2\n   9 │ 1033078                2                        1                         1                  1                            2            1                1                1        5      2\n  10 │ 1033078                4                        2                         1                  1                            2            1                2                1        1      2","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We can see that all the features have been added correctly, we can see that we have an unncessary feature called id, so we will remove it.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"select!(data, Not(:id));\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We also need to remove all the missing data from the DataFrame","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"data = dropmissing(data);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"The class column should be of type categorical, following the MLJ API, so we encode it here.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"transform!(data, :class => categorical, renamecols=false);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Check statistics per column.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"describe(data)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"10×7 DataFrame\n Row │ variable                     mean     min  median  max  nmissing  eltype\n     │ Symbol                       Union…   Any  Union…  Any  Int64     DataType\n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Clump Thickness              4.44217  1    4.0     10          0  Int64\n   2 │ Uniformity of Cell Size      3.15081  1    1.0     10          0  Int64\n   3 │ Uniformity of Cell Shape     3.21523  1    1.0     10          0  Int64\n   4 │ Marginal Adhesion            2.83016  1    1.0     10          0  Int64\n   5 │ Single Epithelial Cell Size  3.23426  1    2.0     10          0  Int64\n   6 │ Bare Nuclei                  3.54466  1    1.0     10          0  Int64\n   7 │ Bland Chromatin              3.4451   1    3.0     10          0  Int64\n   8 │ Normal Nucleoli              2.86969  1    1.0     10          0  Int64\n   9 │ Mitoses                      1.60322  1    1.0     10          0  Int64\n  10 │ class                                 2            4           0  CategoricalValue{Int64,UInt32}","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Split the dataset into training and testing.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"y, X = unpack(data, ==(:class), colname -> true);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We will use only 2/3 for training.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"train, test = partition(eachindex(y), 2 / 3, shuffle=true, rng=rng);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Always remove mean and set the standard deviation to 1.0 when dealing with SVMs.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"stand1 = Standardizer(count=true);\nX = MLJBase.transform(fit!(machine(stand1, X)), X);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"┌ Info: Training \u001b[34mMachine{Standardizer} @008\u001b[39m.\n└ @ MLJBase /home/edwin/.julia/packages/MLJBase/5TNcr/src/machines.jl:319\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Check statistics per column again to ensure standardization, but remember to do it now with the X matrix.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"describe(X)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"9×7 DataFrame\n Row │ variable                     mean          min        median     max      nmissing  eltype\n     │ Symbol                       Float64       Float64    Float64    Float64  Int64     DataType\n─────┼──────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Clump Thickness               6.90029e-17  -1.2203    -0.156754  1.97033         0  Float64\n   2 │ Uniformity of Cell Size      -3.5111e-17   -0.701698  -0.701698  2.23454         0  Float64\n   3 │ Uniformity of Cell Shape     -7.44483e-17  -0.74123   -0.74123   2.27023         0  Float64\n   4 │ Marginal Adhesion             9.96437e-17  -0.638897  -0.638897  2.50294         0  Float64\n   5 │ Single Epithelial Cell Size  -5.29103e-17  -1.00503   -0.555202  3.0434          0  Float64\n   6 │ Bare Nuclei                  -2.77962e-17  -0.698341  -0.698341  1.77157         0  Float64\n   7 │ Bland Chromatin               6.11192e-17  -0.998122  -0.181694  2.6758          0  Float64\n   8 │ Normal Nucleoli              -1.24677e-16  -0.612478  -0.612478  2.33576         0  Float64\n   9 │ Mitoses                       2.17818e-17  -0.348145  -0.348145  4.84614         0  Float64","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Good, now every column has a mean very close to zero, so the standardization was done correctly.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We now create our model with LeastSquaresSVM","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"model = LeastSquaresSVM.LSSVClassifier();\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"These are the values for the hyperparameter grid search. We need to find the best subset from this set of parameters. Although I will not do this here, the best approach is to find a set of good hyperparameters and then refine the search space around that set. That way we can ensure we will always get the best results.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"sigma_values = [0.5, 5.0, 10.0, 15.0, 25.0, 50.0, 100.0, 250.0, 500.0];\nr1 = MLJBase.range(model, :σ, values=sigma_values);\ngamma_values = [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0];\nr2 = MLJBase.range(model, :γ, values=gamma_values);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We now create a TunedModel that will use a 10-folds stratified cross validation scheme in order to find the best set of hyperparameters. The stratification is needed because the classes are somewhat imbalanced:","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Benign: 458 (65.5%)\nMalignant: 241 (34.5%)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"self_tuning_model = TunedModel(\n    model=model,\n    tuning=Grid(rng=rng),\n    resampling=StratifiedCV(nfolds=10),\n    range=[r1, r2],\n    measure=accuracy,\n    acceleration=CPUThreads(), # We use this to enable multithreading\n);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Once the best model is found, we create a machine with it, and fit it","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"mach = machine(self_tuning_model, X, y);\nfit!(mach, rows=train, verbosity=0);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We can now show the best hyperparameters found.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"fitted_params(mach).best_model","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"LSSVClassifier(\n    kernel = :rbf,\n    γ = 0.01,\n    σ = 0.5,\n    degree = 0)\u001b[34m @473\u001b[39m","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"And we test the trained model. We expect somewhere around 94%-96% accuracy.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"results = predict(mach, rows=test);\nacc = accuracy(results, y[test]);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Show the accuracy for the testing set","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"println(acc * 100.0)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"94.73684210526316\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"As you can see, it is fairly easy to use LeastSquaresSVM together with MLJ. We got a good accuracy result and this proves that the implementation is actually correct. This dataset is commonly used as a benchmark dataset to test new algorithms.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Types","page":"Reference","title":"Types","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [LeastSquaresSVM]\nPages = [\"types.jl\"]\nPrivate = false","category":"page"},{"location":"reference/#LeastSquaresSVM.FixedSizeSVR","page":"Reference","title":"LeastSquaresSVM.FixedSizeSVR","text":"FixedSizeSVR <: SVM\nFixedSizeSVR(; kernel=:rbf, γ=1.0, σ=1.0, degree=0)\n\nThe type to hold a Least Squares Support Vector Regressor, using the  fixed size formulation.\n\nFields\n\nkernel::Symbol: The kind of kernel to use for the non-linear mapping of the data. Can be one of the following: :rbf, :linear, or :poly.\nγ::Float64: The gamma hyperparameter that is intrinsic of the Least Squares version of the Support Vector Machines.\nσ::Float64: The hyperparameter for the RBF kernel.\ndegree::Int: The degree of the polynomial kernel. Only used if kernel is :poly.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LeastSquaresSVM.LSSVC","page":"Reference","title":"LeastSquaresSVM.LSSVC","text":"LSSVC <: SVM\nLSSVC(; kernel=:rbf, γ=1.0, σ=1.0, degree=0)\n\nThe type to hold a Least Squares Support Vector Classifier.\n\nFields\n\nkernel::Symbol: The kind of kernel to use for the non-linear mapping of the data. Can be one of the following: :rbf, :linear, or :poly.\nγ::Float64: The gamma hyperparameter that is intrinsic of the Least Squares version of the Support Vector Machines.\nσ::Float64: The hyperparameter for the RBF kernel.\ndegree::Int: The degree of the polynomial kernel. Only used if kernel is :poly.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LeastSquaresSVM.LSSVR","page":"Reference","title":"LeastSquaresSVM.LSSVR","text":"LSSVR <: SVM\nLSSVR(; kernel=:rbf, γ=1.0, σ=1.0, degree=0)\n\nThe type to hold a Least Squares Support Vector Regressor.\n\nFields\n\nkernel::Symbol: The kind of kernel to use for the non-linear mapping of the data. Can be one of the following: :rbf, :linear, or :poly.\nγ::Float64: The gamma hyperparameter that is intrinsic of the Least Squares version of the Support Vector Machines.\nσ::Float64: The hyperparameter for the RBF kernel.\ndegree::Int: The degree of the polynomial kernel. Only used if kernel is :poly.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LeastSquaresSVM.SVM","page":"Reference","title":"LeastSquaresSVM.SVM","text":"SVM\n\nA super type for both classifiers and regressors that are implemented as Support Vector Machines.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Methods","page":"Reference","title":"Methods","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [LeastSquaresSVM]\nPages = [\"training.jl\"]\nPrivate = false","category":"page"},{"location":"reference/#LeastSquaresSVM.svmpredict-Tuple{LSSVC, Any, AbstractMatrix{T} where T}","page":"Reference","title":"LeastSquaresSVM.svmpredict","text":"svmpredict(svm::LSSVC, fits, xnew::AbstractMatrix) -> AbstractArray\n\nUses the information obtained from svmtrain such as the bias and weights to construct a decision function and predict new class values. For the classification problem only.\n\nArguments\n\nsvm::LSSVC: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nfits: It can be any container data structure but it must have four elements: x, the data matrix; y, the labels vector; α, the weights; and b, the bias.\nxnew::AbstractMatrix: The data matrix that contains the new instances to be predicted.\n\nReturns\n\nArray: The labels corresponding to the prediction to each of the instances in xnew.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LeastSquaresSVM.svmpredict-Tuple{LSSVR, Any, AbstractMatrix{T} where T}","page":"Reference","title":"LeastSquaresSVM.svmpredict","text":"svmpredict(svm::LSSVR, fits, xnew::AbstractMatrix) -> AbstractArray\n\nUses the information obtained from svmtrain such as the bias and weights to construct a decision function and predict the new values of the function. For the regression problem only.\n\nArguments\n\nsvm::LSSVR: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nfits: It can be any container data structure but it must have four elements: x, the data matrix; y, the labels vector; α, the weights; and b, the bias.\nxnew::AbstractMatrix: The data matrix that contains the new instances to be predicted.\n\nReturns\n\nArray: The labels corresponding to the prediction to each of the instances in xnew.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LeastSquaresSVM.svmtrain-Tuple{LSSVC, AbstractMatrix{T} where T, AbstractVector{T} where T}","page":"Reference","title":"LeastSquaresSVM.svmtrain","text":"svmtrain(svm::LSSVC, x::AbstractMatrix, y::AbstractVector) -> Tuple\n\nSolves a Least Squares Support Vector classification problem using the Conjugate Gradient method. In particular, it uses the Lanczos process due to the fact that the matrices are symmetric.\n\nArguments\n\nsvm::LSSVC: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nx::AbstractMatrix: The data matrix with the features. It is expected that this array is already standardized, i.e. the mean for each feature is zero and its standard deviation is one.\ny::AbstractVector: A vector that contains the classes. It is expected that there are only two classes, -1 and 1.\n\nReturns\n\nTuple: A tuple containing x, y and the following two elements:\nb: Contains the bias for the decision function.\nα: Contains the weights for the decision function.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LeastSquaresSVM.svmtrain-Tuple{LSSVR, AbstractMatrix{T} where T, AbstractVector{T} where T}","page":"Reference","title":"LeastSquaresSVM.svmtrain","text":"svmtrain(svm::LSSVR, x::AbstractMatrix, y::AbstractVector) -> Tuple\n\nSolves a Least Squares Support Vector regression problem using the Conjugate Gradient method. In particular, it uses the Lanczos process due to the fact that the matrices are symmetric.\n\nArguments\n\nsvm::LSSVR: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nx::AbstractMatrix: The data matrix with the features. It is expected that this array is already standardized, i.e. the mean for each feature is zero and its standard deviation is one.\ny::AbstractVector: A vector that contains the continuous value of the function estimation. The elements can be any subtype of Real.\n\nReturns\n\nTuple: A tuple containing x and the following two elements:\nb: Contains the bias for the decision function.\nα: Contains the weights for the decision function.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = LeastSquaresSVM","category":"page"},{"location":"#LeastSquaresSVM","page":"Home","title":"LeastSquaresSVM","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is LeastSquaresSVM, a Least Squares Support Vector Machine (LSSVM) implementation in pure Julia. It is meant to be used together with the fantastic MLJ.jl Machine Learning framework.","category":"page"},{"location":"#Formulation","page":"Home","title":"Formulation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"It is a re-formulation of the classical Support Vector Machine (SVM) formalism. In this case we attempt to solve a least squares problem which is faster[1], instead of the classic quadratic, convex optimization problem that is solved in the original Support Vector Machine.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the case of LeastSquaresSVM we use the conjugate gradient method, in particular the Lanczos version[2] due to the fact that we solve several linear systems which have the the following structure","category":"page"},{"location":"","page":"Home","title":"Home","text":"A mathbfx = mathbfb","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the matrix A is symmetric.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This fact makes it a great candidate for the Lanczos algorithm, a very fast, iterative procedure based on Krylov subspace methods. The implementation used here is that from the Krylov.jl package.","category":"page"},{"location":"#Rationale","page":"Home","title":"Rationale","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SVM has been the most known and used formulation, but here are some pros and cons for using LSSVMs and LeastSquaresSVM.","category":"page"},{"location":"#Advantages","page":"Home","title":"Advantages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The LSSVM is a great alternative to the classic SVM in the following things:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Solving a linear system is much easier and faster than solving a quadratic optimization problem.\nSome useful properties from numerical linear algebra can be exploited in order to solve the new optimization problem.\nOne can potentialy train of thousands or millions of instances using LSSVM, something that the classic SVM cannot do. This is possible using the fixed size LSSVM[3].\nLess hyperparemeters to tune. LSSVM only has one intrinsic hyperparamter, whereas the SVM has at least two. This is without taking into account the kernel's hyperparameter.","category":"page"},{"location":"#Disadvantages","page":"Home","title":"Disadvantages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"But there are some important shortcommings for the LSSVM, namely:","category":"page"},{"location":"","page":"Home","title":"Home","text":"In contrast with the classic SVM, the decision function lack all sparseness. Every single dataset instance must be used to train the LSSVM. This can become troublesome for very large problems because all the instances must fit into memory.\nThere is complete lack of interpretation for the support vectors, which are the data instances that are used to construct the decision function. Because all data instances are used, every instance is effectively a support vector and this removes any interpretation for the importance of each instance on the model's performance.","category":"page"},{"location":"#Bibliography","page":"Home","title":"Bibliography","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[1]: Suykens, J. A., & Vandewalle, J. (1999). Least squares support vector machine classifiers. Neural processing letters, 9(3), 293-300.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[2]: Fasano, G. (2007). Lanczos conjugate-gradient method and pseudoinverse computation on indefinite and singular systems. Journal of optimization theory and applications, 132(2), 267-285.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[3]: Espinoza, M., Suykens, J. A., & De Moor, B. (2006). Fixed-size least squares support vector machines: A large scale application in electrical load forecasting. Computational Management Science, 3(2), 113-129.","category":"page"}]
}
