<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Multiclass classification · LeastSquaresSVM</title><link rel="canonical" href="https://edwinb-ai.github.io/LeastSquaresSVM/stable/example4/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">LeastSquaresSVM</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../example1/">Classification of the Wisconsin breast cancer dataset</a></li><li><a class="tocitem" href="../example2/">Regression on a synthetic dataset</a></li><li><a class="tocitem" href="../example3/">Using different kernels</a></li><li class="is-active"><a class="tocitem" href>Multiclass classification</a><ul class="internal"><li><a class="tocitem" href="#Conclusions"><span>Conclusions</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Multiclass classification</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Multiclass classification</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Multiclass-classification"><a class="docs-heading-anchor" href="#Multiclass-classification">Multiclass classification</a><a id="Multiclass-classification-1"></a><a class="docs-heading-anchor-permalink" href="#Multiclass-classification" title="Permalink"></a></h1><p>As of version <code>v0.7</code> and over, the <code>LSSVClassifier</code> can handle multiclass classification problems. It does so by using the so-called <em>one-vs-one approach</em>. This is described as follows:</p><ul><li>For a classification problem with <span>$k$</span> classes, one trains <span>$k(k-1)/2$</span> binary classifiers.</li><li>Each binary classifier deals with every possible class pair combination. For instance, for a three class classification problem, we need to train three classifiers. The first will deal with the (1, 2) binary classification problem. The second with the (1, 3), while the third classifier will deal with the (2, 3) class pair.</li><li>We then save all these parameters, and on the prediction step, we ask every classifier to predict a given data instance. Care must be taken to ensure that the class pairs are decoded correctly.</li><li>A <em>voting scheme</em> is then applied to these predictions. The class that has the largest number of votes when a pediction is carried out is the class that is taken as the predicted class. In the important case of a tie, the predicted class is always that of the lowest index. This is obviously not a great strategy, but it&#39;s simple enough to break the tie. See<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> to read more about this strategy.</li></ul><p>Here, we deal with the binary classification problems with the underlying implementation of the <code>LSSVClassifier</code> and finally pool all of our results together. We then apply the voting scheme and we are done.</p><p>In this example we will showcase this ability on the very famous and not so difficult problem of the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris flower dataset.</a></p><p>This problem is a 3-class classification problem, with 4 features and a total of 150 samples; 50 for each class. It is expected that with a good set of hyperparameters, our implementation should easily obtain a 100% accuracy classification rate on this problem.</p><p>Let us begin by importing our packages and setting an RNG to ensure reproducibility.</p><pre><code class="language-julia">using MLJ, MLJBase
using Random
using LeastSquaresSVM

rng = MersenneTwister(957);
</code></pre><p>The Iris dataset is so famous, it is included in the <code>MLJ.jl</code> package. By using the very convenient macro <code>@load_iris</code> we can obtain the full dataset, as follows.</p><pre><code class="language-julia">X, y = @load_iris;
</code></pre><p>Then, we split the dataset. We have said before that the datset is small, so we will keep this in mind. A 60-40 train-test split should suffice.</p><pre><code class="language-julia">train, test = partition(eachindex(y), 0.6, shuffle=true, rng=rng);
</code></pre><p>We will now instantiate our model. Recall that the default kernel is the RBF kernel.</p><pre><code class="language-julia">model = LSSVClassifier();
</code></pre><p>Now we set some ranges for the hyperparameters and also instantiate a self-tuning model Note however that a <code>repeats</code> option is now used. What this does is to repeat the resampling of the dataset that many times. This is just to ensure that the model is learning correctly and not being biased.</p><pre><code class="language-julia">r1 = range(model, :σ, lower=1e-2, upper=1e-1);
r2 = range(model, :γ, lower=140, upper=150);
self_tuning_model = TunedModel(
    model=model,
    tuning=Grid(goal=200, rng=rng),
    resampling=StratifiedCV(nfolds=5),
    range=[r1, r2],
    measure=accuracy,
    acceleration=CPUThreads(),
    repeats=10 # Add more resampling to be sure we are not biased
);
</code></pre><p>We will use a pipeline to first standardize the data and then feed it to the model.</p><pre><code class="language-julia">pipe = @pipeline(Standardizer(), self_tuning_model);
mach = MLJ.machine(pipe, X, y);
</code></pre><p>It&#39;s time to train. Remeber that if you are going to use multithreading for the hyperparameter search, you need to start your <code>Julia</code> session with the environment variable <code>JULIA_NUM_THREADS</code> set to a different number than 1.</p><pre><code class="language-julia">MLJ.fit!(mach, rows=train, verbosity=0);
</code></pre><p>We want to see the best set of hyperparameters.</p><pre><code class="language-julia">fitted_params(mach).deterministic_tuned_model.best_model</code></pre><pre><code class="language-none">LSSVClassifier(
    kernel = :rbf,
    γ = 147.69230769230768,
    σ = 0.01,
    degree = 0)[34m @315[39m</code></pre><p>Great! Let us see how good was the model. Let&#39;s check with the test set.</p><pre><code class="language-julia">results = MLJ.predict(mach, rows=test);
acc = MLJ.accuracy(results, y[test]);
acc</code></pre><pre><code class="language-none">1.0</code></pre><p>Fantastic! A 100% accuracy, as was expected. As a good measure, let us inspect the confusion matrix of the classification problem.</p><pre><code class="language-julia">results = coerce(results, OrderedFactor);
y_ordered = coerce(y[test], OrderedFactor);
confusion_matrix(results, y_ordered)</code></pre><pre><code class="language-none">              ┌─────────────────────────────────────────┐
              │              Ground Truth               │
┌─────────────┼─────────────┬─────────────┬─────────────┤
│  Predicted  │   setosa    │  versicol…  │  virginica  │
├─────────────┼─────────────┼─────────────┼─────────────┤
│   setosa    │     19      │      0      │      0      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│  versicol…  │      0      │     21      │      0      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│  virginica  │      0      │      0      │     20      │
└─────────────┴─────────────┴─────────────┴─────────────┘
</code></pre><h2 id="Conclusions"><a class="docs-heading-anchor" href="#Conclusions">Conclusions</a><a id="Conclusions-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions" title="Permalink"></a></h2><p>Again, just as expected, the result is perfect. But do note that this does not mean that the implementation or the model is the best there is. There are other models and implementations that should be checked along with this one.</p><p>As a good rule of thumb, when doing machine learning, always try to train <em>at least</em> one more model to compare performance.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Chih-Wei Hsu and Chih-Jen Lin (2002) ‘A comparison of methods for multiclass support vector machines’, IEEE Transactions on Neural Networks, 13(2), pp. 416. doi: 10.1109/72.991427.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../example3/">« Using different kernels</a><a class="docs-footer-nextpage" href="../reference/">Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 27 September 2021 01:47">Monday 27 September 2021</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
