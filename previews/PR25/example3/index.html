<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Using different kernels Â· Elysivm</title><link rel="canonical" href="https://edwinb-ai.github.io/Elysivm/stable/example3/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Elysivm</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../example1/">Classification of the Wisconsin breast cancer dataset</a></li><li><a class="tocitem" href="../example2/">Regression on a synthetic dataset</a></li><li class="is-active"><a class="tocitem" href>Using different kernels</a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Using different kernels</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Using different kernels</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Using-different-kernels"><a class="docs-heading-anchor" href="#Using-different-kernels">Using different kernels</a><a id="Using-different-kernels-1"></a><a class="docs-heading-anchor-permalink" href="#Using-different-kernels" title="Permalink"></a></h1><p>Starting with version v0.6, two new kernels can be chosen: a <strong>linear</strong> kernel and a <strong>polynomial</strong> kernel. In this document we will see how to handle choosing a different kernel, and we&#39;ll showcase an example.</p><p>First, we need to import all the necessary packages.</p><pre><code class="language-julia">using Elysivm
using MLJ, MLJBase
using DataFrames, CSV
using CategoricalArrays, Random
using Plots
gr();
rng = MersenneTwister(812);
nothing #hide</code></pre><p>For this example, we will create a large classification problem. It is actually inspired from a similar classification problem from <a href="https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py"><code>scikit-learn</code></a>.</p><p>The idea is to have a very large number of features (5000), and a small number of instances. This has been <a href="https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf">reported</a> as as being a good use case or rule of thumb</p><blockquote><p>Whenever the number of features is <em>larger</em> than the number of instances, use a linear kernel.</p></blockquote><pre><code class="language-julia">X, y = MLJ.make_blobs(500, 2_000; centers=2, cluster_std=[1.5, 0.5]);
nothing #hide</code></pre><p>Of course, this is just to showcase the implementation within <code>Elysivm</code>. There are actually better ways to handle this kind of problem, e.g. dimensionality-reduction algorithms.</p><p>The <code>make_blobs</code> function is very similar to that of <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs"><code>scikit-learn</code>s</a>. The idea is to create circles, or clusters, and to classify between them.</p><p>We&#39;ll use them to test our <em>linear</em> kernel.</p><p>We need to construct a <code>DataFrame</code> with the arrays created to better handle the data, as well as a better integration with <code>MLJ</code>.</p><pre><code class="language-julia">df = DataFrame(X);
df.y = y;
nothing #hide</code></pre><p>Recall that we need to change the primitive types of <code>Julia</code> to <code>scitypes</code>.</p><pre><code class="language-julia">dfnew = coerce(df, autotype(df));
nothing #hide</code></pre><p>We can then observe the first three columns, together with their new types. We&#39;ll just look at the first 8 features to avoid cluttering the space.</p><pre><code class="language-julia">first(dfnew[:, 1:8], 3) |&gt; pretty</code></pre><pre><code class="language-none">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ x1         â”‚ x2         â”‚ x3         â”‚ x4         â”‚ x5         â”‚ x6         â”‚ x7         â”‚ x8         â”‚
â”‚ Float64    â”‚ Float64    â”‚ Float64    â”‚ Float64    â”‚ Float64    â”‚ Float64    â”‚ Float64    â”‚ Float64    â”‚
â”‚ Continuous â”‚ Continuous â”‚ Continuous â”‚ Continuous â”‚ Continuous â”‚ Continuous â”‚ Continuous â”‚ Continuous â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4.6069     â”‚ -11.0476   â”‚ 9.02517    â”‚ 2.06608    â”‚ 10.3952    â”‚ 8.74835    â”‚ 0.534613   â”‚ -0.493674  â”‚
â”‚ -7.47441   â”‚ 1.80263    â”‚ -0.47754   â”‚ 5.36791    â”‚ -9.11338   â”‚ -2.67125   â”‚ -4.20552   â”‚ -1.68881   â”‚
â”‚ -6.90708   â”‚ 1.61288    â”‚ -0.852302  â”‚ 5.24683    â”‚ -9.36584   â”‚ -3.15882   â”‚ -6.1735    â”‚ -1.85193   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre><p>We should also check out the basic statistics of the dataset. We&#39;ll only use a small subset as the data frame it quite large.</p><pre><code class="language-julia">describe(dfnew[1:20, 1:10], :mean, :std, :eltype)</code></pre><pre><code class="language-none">10Ã—4 DataFrame
 Row â”‚ variable  mean       std      eltype
     â”‚ Symbol    Float64    Float64  DataType
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1 â”‚ x1         0.768153  6.07188  Float64
   2 â”‚ x2        -5.57183   5.79931  Float64
   3 â”‚ x3         5.38725   4.93904  Float64
   4 â”‚ x4         3.94973   1.804    Float64
   5 â”‚ x5         3.24621   9.60158  Float64
   6 â”‚ x6         3.47164   4.90996  Float64
   7 â”‚ x7        -1.08106   3.11007  Float64
   8 â”‚ x8        -0.9725    1.10477  Float64
   9 â”‚ x9         7.51147   1.06876  Float64
  10 â”‚ x10       -0.735586  7.56975  Float64</code></pre><p>Recall that we also need to standardize the dataset, we can see here that the mean is close to zero, but not quite, and we also need an unitary standard deviation.</p><p>Split the dataset into training and testing sets.</p><pre><code class="language-julia">y, X = unpack(dfnew, ==(:y), colname -&gt; true);
train, test = partition(eachindex(y), 0.75, shuffle=true, rng=rng);
stand1 = Standardizer();
X = MLJBase.transform(MLJBase.fit!(MLJBase.machine(stand1, X)), X);
nothing #hide</code></pre><pre><code class="language-none">â”Œ Info: Training [34mMachine{Standardizer} @835[39m.
â”” @ MLJBase /home/edwin/.julia/packages/MLJBase/vwzmG/src/machines.jl:319
</code></pre><p>We should make sure that the features have mean close to zero and an unitary standard deviation. Again, using only a small subset.</p><pre><code class="language-julia">X_df = DataFrame(X)
describe(X_df[1:20, 1:10], :mean, :std, :eltype)</code></pre><pre><code class="language-none">10Ã—4 DataFrame
 Row â”‚ variable  mean       std       eltype
     â”‚ Symbol    Float64    Float64   DataType
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1 â”‚ x1         0.29838   0.97679   Float64
   2 â”‚ x2        -0.258993  0.964678  Float64
   3 â”‚ x3         0.253288  1.00743   Float64
   4 â”‚ x4        -0.341693  1.09002   Float64
   5 â”‚ x5         0.311695  0.967953  Float64
   6 â”‚ x6         0.390866  1.06769   Float64
   7 â”‚ x7         0.248482  1.00987   Float64
   8 â”‚ x8         0.299902  1.04662   Float64
   9 â”‚ x9         0.335588  0.900707  Float64
  10 â”‚ x10       -0.317464  0.977763  Float64</code></pre><p>For the case of a <em>linear</em> kernel, no hyperparameter is needed. Instead, the only hyperparameter that needs to be adjusted is the <span>$\gamma$</span> value that is intrinsic of the least-squares formulation. We will search for a good hyperparameter now.</p><p>We will use the <code>accuracy</code> as a metric. The accuracy is simply defined as</p><div>\[\text{accuracy} = \frac{\text{number of correct predictions}}{\text{total number of predictions}}\]</div><p>Note that the accuracy is not always a good measure of classification, but it will do fine on this dataset.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Remember that the least-squares formulation uses <strong>all</strong> the data samples, so the following will actually consume at least &gt; 6 GB of RAM. Do not run this on your hardware if you are not sure you have this kind of resources available. At the very least, replace <code>CPUThreads()</code> with <code>CPU1()</code> to disable multithreading. Methods to handle memory more efficiently will be available in future versions.</p></div></div><pre><code class="language-julia">model = LSSVClassifier(kernel=&quot;linear&quot;);
r1 = range(model, :Ïƒ, lower=1.0, upper=1000.0);
self_tuning_model = TunedModel(
    model=model,
    tuning=Grid(goal=400, rng=rng),
    resampling=StratifiedCV(nfolds=5),
    range=[r1],
    measure=accuracy,
    acceleration=CPUThreads(), # We use this to enable multithreading
);
nothing #hide</code></pre><p>And now we proceed to train all the models and find the best one!</p><pre><code class="language-julia">mach = machine(self_tuning_model, X, y);
fit!(mach, rows=train, verbosity=0);
fitted_params(mach).best_model</code></pre><pre><code class="language-none">LSSVClassifier(
    kernel = &quot;linear&quot;,
    Î³ = 1.0,
    Ïƒ = 283.9248120300752,
    degree = 0)[34m @161[39m</code></pre><p>Having found the best hyperparameters for the regressor model we proceed to check how the model generalizes and we use the test set to check the performance.</p><pre><code class="language-julia">yÌ‚ = MLJBase.predict(mach, rows=test);
result = accuracy(yÌ‚, y[test])
@show result # Check th</code></pre><pre><code class="language-none">1.0</code></pre><p>We can see that we did quite well. A value of 1, or close enough, means the classifier is <em>perfect.</em> That is, it can classify correctly between each class.</p><p>Finally, let us look at the so-called <em>confusion matrix.</em> This table shows us useful information about the performance of our classifier.</p><p>Let us compute it first, and then we&#39;ll analyse it. Notice, however, that we need to first coerce the types to <code>OrderedFactor</code> <em>scitypes</em> in order for the confusion matrix to be computed correctly.</p><pre><code class="language-julia">yÌ‚ = coerce(yÌ‚, OrderedFactor);
y_ordered = coerce(y[test], OrderedFactor);
confusion_matrix(yÌ‚, y_ordered)</code></pre><pre><code class="language-none">              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚       Ground Truth        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Predicted  â”‚      1      â”‚      2      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      1      â”‚     52      â”‚      0      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      2      â”‚      0      â”‚     73      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre><p>The way you read the confusion matrix is the following. The main diagonal tells us how many correct predictions were obtained by the classifier for both classes. On the other hand, the other values are the following</p><ul><li>The <em>upper right</em> value is known as the <strong>false positive</strong>. This is the number of instances that were classified as belonging to a given class, when actually they were instances of the other one. An example would be if we had an instance <span>$x_1$</span> which belonged to the class <code>b</code>, but the classifier would have predicted it actually belonged to class <code>a</code>.</li><li>The <em>lower left</em> value is known as the <strong>false negative</strong>. This is the number of instances classified as <em>not</em> belonging to a given class, when they actually belonged to a class. An example would be if we had an instance <span>$x_2$</span> which belonged to class <code>a</code>, and the classifier actually predicted it belonged to class <code>b</code>.</li></ul><p>It might be a little bit confusing, so a good starting point for more information on the subject is the excellent <a href="https://en.wikipedia.org/wiki/Confusion_matrix">Wikipedia article.</a> You might also be interested in the following <a href="https://developers.google.com/machine-learning/crash-course/classification/accuracy">document</a> from a Google&#39;s Machine Learning Crash Course.</p><p>In this case, we can see that no false negative or positive values were found, which means that the classifier did outstandingly good. Normally, we can expect to have at least some percentage of false negative or positives.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../example2/">Â« Regression on a synthetic dataset</a><a class="docs-footer-nextpage" href="../reference/">Reference Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 20 January 2021 04:22">Wednesday 20 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
