var documenterSearchIndex = {"docs":
[{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"EditURL = \"<unknown>/src/examples/example2.jl\"","category":"page"},{"location":"example2/#Regression-on-a-synthetic-dataset","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"","category":"section"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"In this page we will see how to perform Least Squares Support Vector Regression using Elysivm. To accomplish this task, we will use synthetic data as created by the make_regression function from MLJ.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"First, we need to import all the necessary packages.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"using Elysivm\nusing MLJ, MLJBase\nusing DataFrames, CSV\nusing CategoricalArrays, Random\nusing Plots\ngr();\nrng = MersenneTwister(88);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We then create the regression problem. To really push the implementation we will create a problem with 5 features and 500 instances/observations. We will also add a little bit of Gaussian noise to the problem.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"X, y = MLJ.make_regression(500, 5; noise=1.0, rng=rng);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We need to construct a DataFrame with the arrays created to better handle the data, as well as a better integration with MLJ.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"df = DataFrame(X);\ndf.y = y;\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"A very important part of the MLJ framework is its use of scitypes, a special kind of type that work together with the objects from the framework. Because the regression problem has the Julia types we need to convert this types to correct scitypes such such that the machines from MLJ work fine.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"dfnew = coerce(df, autotype(df));\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can then observe the first three columns, together with their new types.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"first(dfnew, 3) |> pretty","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We should also check out the basic statistics of the dataset.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"describe(dfnew, :mean, :std, :eltype)","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Recall that we also need to standardize the dataset, we can see here that the mean is close to zero, but not quite, and we also need an unitary standard deviation.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Split the dataset into training and testing sets.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"y, X = unpack(dfnew, ==(:y), colname -> true);\ntrain, test = partition(eachindex(y), 0.75, shuffle=true, rng=rng);\nstand1 = Standardizer();\nX = MLJBase.transform(MLJBase.fit!(MLJBase.machine(stand1, X)), X);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We should make sure that the features have mean close to zero and an unitary standard deviation.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"describe(X |> DataFrame, :mean, :std, :eltype)","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Define a good set of hyperparameters for this problem and train the regressor. We will use the amazing capability of MLJ to tune a model and return the best model found.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"For this we have taken some judicious guessing on the best values that the hyperparameters should take. We employ 5-fold cross-validation and a 400 by 400 grid of points to do an exhaustive search.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We will train the regressor using the root mean square error which is defined as follows","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"RMSE = sqrtfracsum_i=1^N left(haty_i - y_i right)^2N","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"where we define haty_i as the predicted value, and y_i as the real value.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"model = Elysivm.LSSVRegressor();\nr1 = MLJBase.range(model, :σ, lower=7e-4, upper=1e-3);\nr2 = MLJBase.range(model, :γ, lower=120.0, upper=130.0);\nself_tuning_model = TunedModel(\n    model=model,\n    tuning=Grid(goal=400, rng=rng),\n    resampling=CV(nfolds=5),\n    range=[r1, r2],\n    measure=MLJBase.rms,\n    acceleration=CPUThreads(), # We use this to enable multithreading\n);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"And now we proceed to train all the models and find the best one!","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"mach = MLJ.machine(self_tuning_model, X, y);\nMLJBase.fit!(mach, rows=train, verbosity=0);\nfitted_params(mach).best_model","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Having found the best hyperparameters for the regressor model we proceed to check how the model generalizes and we use the test set to check the performance.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"ŷ = MLJBase.predict(mach, rows=test);\nresult = round(MLJBase.rms(ŷ, y[test]), sigdigits=4);\n@show result # Check the result","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can see that we did quite well. A value of 1, or close enough, is good. We expect it to reach a lower value, closer to zero, but maybe we needed more refinement in the grid search.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can also see a plot of the predicted and true values. The closer these dots are to the diagonal means that the model performed well.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"scatter(ŷ, y[test], markersize=9)\nr = range(minimum(y[test]), maximum(y[test]); length=length(test))\nplot!(r, r, linewidth=9)\nplot!(size=(3000, 2100))","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can actually see that we are not that far off, maybe a little more search could definitely improve the performance of our model.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"This page was generated using Literate.jl.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"EditURL = \"<unknown>/src/examples/example3.jl\"","category":"page"},{"location":"example3/#Using-different-kernels","page":"Using different kernels","title":"Using different kernels","text":"","category":"section"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Starting with version v0.6, two new kernels can be chosen: a linear kernel and a polynomial kernel. In this document we will see how to handle choosing a different kernel, and we'll showcase an example.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"First, we need to import all the necessary packages.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"using Elysivm\nusing MLJ, MLJBase\nusing DataFrames, CSV\nusing CategoricalArrays, Random\nusing Plots\ngr();\nrng = MersenneTwister(812);\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"For this example, we will create a large classification problem. It is actually inspired from a similar classification problem from scikit-learn.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"The idea is to have a very large number of features (5000), and a small number of instances. This has been reported as as being a good use case or rule of thumb","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Whenever the number of features is larger than the number of instances, use a linear kernel.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"X, y = MLJ.make_blobs(500, 2_000; centers=2, cluster_std=[1.5, 0.5]);\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Of course, this is just to showcase the implementation within Elysivm. There are actually better ways to handle this kind of problem, e.g. dimensionality-reduction algorithms.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"The make_blobs function is very similar to that of scikit-learns. The idea is to create circles, or clusters, and to classify between them.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We'll use them to test our linear kernel.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We need to construct a DataFrame with the arrays created to better handle the data, as well as a better integration with MLJ.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"df = DataFrame(X);\ndf.y = y;\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Recall that we need to change the primitive types of Julia to scitypes.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"dfnew = coerce(df, autotype(df));\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We can then observe the first three columns, together with their new types. We'll just look at the first 8 features to avoid cluttering the space.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"first(dfnew[:, 1:8], 3) |> pretty","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"┌────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┐\n│ x1         │ x2         │ x3         │ x4         │ x5         │ x6         │ x7         │ x8         │\n│ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │\n│ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │\n├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n│ -1.83751   │ -8.48306   │ 4.28848    │ 5.58993    │ 9.24882    │ -9.12689   │ -0.696171  │ -0.460113  │\n│ -1.75552   │ -7.2933    │ 2.96862    │ 5.83471    │ 8.86175    │ -9.40532   │ -1.41453   │ -0.710507  │\n│ -9.09264   │ -8.53732   │ -6.44803   │ 9.51166    │ 7.89959    │ 11.4616    │ -0.481532  │ -5.18937   │\n└────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┘\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We should also check out the basic statistics of the dataset. We'll only use a small subset as the data frame it quite large.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"describe(dfnew[1:20, 1:10], :mean, :std, :eltype)","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"10×4 DataFrame\n Row │ variable  mean       std       eltype\n     │ Symbol    Float64    Float64   DataType\n─────┼─────────────────────────────────────────\n   1 │ x1        -4.97356   3.02476   Float64\n   2 │ x2        -7.32772   1.28577   Float64\n   3 │ x3        -2.95273   6.13712   Float64\n   4 │ x4         7.34868   2.09713   Float64\n   5 │ x5         6.77488   2.15255   Float64\n   6 │ x6         0.745583  9.62697   Float64\n   7 │ x7        -0.928026  0.847911  Float64\n   8 │ x8        -4.12216   3.04683   Float64\n   9 │ x9         1.24296   3.78326   Float64\n  10 │ x10       -0.396007  7.00897   Float64","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Recall that we also need to standardize the dataset, we can see here that the mean is close to zero, but not quite, and we also need an unitary standard deviation.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Split the dataset into training and testing sets.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"y, X = unpack(dfnew, ==(:y), colname -> true);\ntrain, test = partition(eachindex(y), 0.75, shuffle=true, rng=rng);\nstand1 = Standardizer();\nX = MLJBase.transform(MLJBase.fit!(MLJBase.machine(stand1, X)), X);\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"┌ Info: Training \u001b[34mMachine{Standardizer} @093\u001b[39m.\n└ @ MLJBase /home/edwin/.julia/packages/MLJBase/5TNcr/src/machines.jl:319\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We should make sure that the features have mean close to zero and an unitary standard deviation. Again, using only a small subset.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"X_df = DataFrame(X)\ndescribe(X_df[1:20, 1:10], :mean, :std, :eltype)","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"10×4 DataFrame\n Row │ variable  mean        std       eltype\n     │ Symbol    Float64     Float64   DataType\n─────┼──────────────────────────────────────────\n   1 │ x1        -0.133188   1.10637   Float64\n   2 │ x2         0.518277   1.10013   Float64\n   3 │ x3        -0.0332144  0.920852  Float64\n   4 │ x4        -0.003687   0.948112  Float64\n   5 │ x5         0.0116106  0.876971  Float64\n   6 │ x6         0.178795   1.0669    Float64\n   7 │ x7        -0.0214703  0.780705  Float64\n   8 │ x8        -0.160642   1.09445   Float64\n   9 │ x9        -0.0385091  0.925363  Float64\n  10 │ x10       -0.165552   1.08708   Float64","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"For the case of a linear kernel, no hyperparameter is needed. Instead, the only hyperparameter that needs to be adjusted is the gamma value that is intrinsic of the least-squares formulation. We will search for a good hyperparameter now.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We will use the accuracy as a metric. The accuracy is simply defined as","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"textaccuracy = fractextnumber of correct predictionstexttotal number of predictions","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Note that the accuracy is not always a good measure of classification, but it will do fine on this dataset.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"warning: Warning\nRemember that the least-squares formulation uses all the data samples, so the following will actually consume at least > 6 GB of RAM. Do not run this on your hardware if you are not sure you have this kind of resources available. At the very least, replace CPUThreads() with CPU1() to disable multithreading. Methods to handle memory more efficiently will be available in future versions.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"model = LSSVClassifier(kernel=:linear);\nr1 = range(model, :σ, lower=1.0, upper=1000.0);\nself_tuning_model = TunedModel(\n    model=model,\n    tuning=Grid(goal=400, rng=rng),\n    resampling=StratifiedCV(nfolds=5),\n    range=[r1],\n    measure=accuracy,\n    acceleration=CPUThreads(), # We use this to enable multithreading\n);\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"And now we proceed to train all the models and find the best one!","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"mach = machine(self_tuning_model, X, y);\nfit!(mach, rows=train, verbosity=0);\nfitted_params(mach).best_model","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"LSSVClassifier(\n    kernel = :linear,\n    γ = 1.0,\n    σ = 283.9248120300752,\n    degree = 0)\u001b[34m @551\u001b[39m","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Having found the best hyperparameters for the regressor model we proceed to check how the model generalizes and we use the test set to check the performance.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"ŷ = MLJBase.predict(mach, rows=test);\nresult = accuracy(ŷ, y[test])\n@show result # Check th","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"1.0","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"We can see that we did quite well. A value of 1, or close enough, means the classifier is perfect. That is, it can classify correctly between each class.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Finally, let us look at the so-called confusion matrix. This table shows us useful information about the performance of our classifier.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"Let us compute it first, and then we'll analyse it. Notice, however, that we need to first coerce the types to OrderedFactor scitypes in order for the confusion matrix to be computed correctly.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"ŷ = coerce(ŷ, OrderedFactor);\ny_ordered = coerce(y[test], OrderedFactor);\nconfusion_matrix(ŷ, y_ordered)","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"              ┌───────────────────────────┐\n              │       Ground Truth        │\n┌─────────────┼─────────────┬─────────────┤\n│  Predicted  │      1      │      2      │\n├─────────────┼─────────────┼─────────────┤\n│      1      │     59      │      0      │\n├─────────────┼─────────────┼─────────────┤\n│      2      │      0      │     66      │\n└─────────────┴─────────────┴─────────────┘\n","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"The way you read the confusion matrix is the following. The main diagonal tells us how many correct predictions were obtained by the classifier for both classes. On the other hand, the other values are the following","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"The upper right value is known as the false positive. This is the number of instances that were classified as belonging to a given class, when actually they were instances of the other one. An example would be if we had an instance x_1 which belonged to the class b, but the classifier would have predicted it actually belonged to class a.\nThe lower left value is known as the false negative. This is the number of instances classified as not belonging to a given class, when they actually belonged to a class. An example would be if we had an instance x_2 which belonged to class a, and the classifier actually predicted it belonged to class b.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"It might be a little bit confusing, so a good starting point for more information on the subject is the excellent Wikipedia article. You might also be interested in the following document from a Google's Machine Learning Crash Course.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"In this case, we can see that no false negative or positive values were found, which means that the classifier did outstandingly good. Normally, we can expect to have at least some percentage of false negative or positives.","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"","category":"page"},{"location":"example3/","page":"Using different kernels","title":"Using different kernels","text":"This page was generated using Literate.jl.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"EditURL = \"<unknown>/src/examples/example1.jl\"","category":"page"},{"location":"example1/#Classification-of-the-Wisconsin-breast-cancer-dataset","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"","category":"section"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"In this case study we will deal with the Wisconsin breast cancer dataset which can be browsed freely on the UCI website.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"In particular, this dataset contains 10 features and 699 instances. In the work we will do here, however, we will skip some instances due to some missing values.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"The dataset contains only two classes, and the purpose is to use all ten features to answer a simple question:","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Does the subject have a benign or malign tumor?","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"To answer this question, we will train a Least Squares Support Vector Machine as implemented in Elysivm.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"First, we need to import all the necessary packages.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"using MLJ, MLJBase\nusing DataFrames, CSV\nusing CategoricalArrays\nusing Random, Statistics\nusing Elysivm","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We then need to specify a seed to enable reproducibility of the results.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"rng = MersenneTwister(801239);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Here we are creating a list with all the headers.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"headers = [\n\t\"id\", \"Clump Thickness\",\n\t\"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\n\t\"Marginal Adhesion\", \"Single Epithelial Cell Size\",\n\t\"Bare Nuclei\", \"Bland Chromatin\",\n\t\"Normal Nucleoli\", \"Mitoses\", \"class\"\n];\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We define the path were the dataset is located","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"path = joinpath(\"src\", \"examples\", \"wbc.csv\");\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We load the csv file and convert it to a DataFrame. Note that we are specifying to the file reader to replace the string ? to a missing value. This dataset contains the the string ? when there is a value missing.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"data = CSV.File(path; header=headers, missingstring=\"?\") |> DataFrame;\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We can display the first 10 rows from the dataset","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"first(data, 10)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"10×11 DataFrame\n Row │ id       Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  class\n     │ Int64    Int64            Int64                    Int64                     Int64              Int64                        Int64?       Int64            Int64            Int64    Int64\n─────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ 1000025                5                        1                         1                  1                            2            1                3                1        1      2\n   2 │ 1002945                5                        4                         4                  5                            7           10                3                2        1      2\n   3 │ 1015425                3                        1                         1                  1                            2            2                3                1        1      2\n   4 │ 1016277                6                        8                         8                  1                            3            4                3                7        1      2\n   5 │ 1017023                4                        1                         1                  3                            2            1                3                1        1      2\n   6 │ 1017122                8                       10                        10                  8                            7           10                9                7        1      4\n   7 │ 1018099                1                        1                         1                  1                            2           10                3                1        1      2\n   8 │ 1018561                2                        1                         2                  1                            2            1                3                1        1      2\n   9 │ 1033078                2                        1                         1                  1                            2            1                1                1        5      2\n  10 │ 1033078                4                        2                         1                  1                            2            1                2                1        1      2","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We can see that all the features have been added correctly, we can see that we have an unncessary feature called id, so we will remove it.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"select!(data, Not(:id));\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We also need to remove all the missing data from the DataFrame","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"data = dropmissing(data);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"The class column should be of type categorical, following the MLJ API, so we encode it here.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"transform!(data, :class => categorical, renamecols=false);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Check statistics per column.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"describe(data)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"10×7 DataFrame\n Row │ variable                     mean     min  median  max  nmissing  eltype\n     │ Symbol                       Union…   Any  Union…  Any  Int64     DataType\n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Clump Thickness              4.44217  1    4.0     10          0  Int64\n   2 │ Uniformity of Cell Size      3.15081  1    1.0     10          0  Int64\n   3 │ Uniformity of Cell Shape     3.21523  1    1.0     10          0  Int64\n   4 │ Marginal Adhesion            2.83016  1    1.0     10          0  Int64\n   5 │ Single Epithelial Cell Size  3.23426  1    2.0     10          0  Int64\n   6 │ Bare Nuclei                  3.54466  1    1.0     10          0  Int64\n   7 │ Bland Chromatin              3.4451   1    3.0     10          0  Int64\n   8 │ Normal Nucleoli              2.86969  1    1.0     10          0  Int64\n   9 │ Mitoses                      1.60322  1    1.0     10          0  Int64\n  10 │ class                                 2            4           0  CategoricalValue{Int64,UInt32}","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Split the dataset into training and testing.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"y, X = unpack(data, ==(:class), colname -> true);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We will use only 2/3 for training.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"train, test = partition(eachindex(y), 2 / 3, shuffle=true, rng=rng);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Always remove mean and set the standard deviation to 1.0 when dealing with SVMs.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"stand1 = Standardizer(count=true);\nX = MLJBase.transform(fit!(machine(stand1, X)), X);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"┌ Info: Training \u001b[34mMachine{Standardizer} @648\u001b[39m.\n└ @ MLJBase /home/edwin/.julia/packages/MLJBase/5TNcr/src/machines.jl:319\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Check statistics per column again to ensure standardization, but remember to do it now with the X matrix.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"describe(X)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"9×7 DataFrame\n Row │ variable                     mean          min        median     max      nmissing  eltype\n     │ Symbol                       Float64       Float64    Float64    Float64  Int64     DataType\n─────┼──────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Clump Thickness               6.90029e-17  -1.2203    -0.156754  1.97033         0  Float64\n   2 │ Uniformity of Cell Size      -3.5111e-17   -0.701698  -0.701698  2.23454         0  Float64\n   3 │ Uniformity of Cell Shape     -7.44483e-17  -0.74123   -0.74123   2.27023         0  Float64\n   4 │ Marginal Adhesion             9.96437e-17  -0.638897  -0.638897  2.50294         0  Float64\n   5 │ Single Epithelial Cell Size  -5.29103e-17  -1.00503   -0.555202  3.0434          0  Float64\n   6 │ Bare Nuclei                  -2.77962e-17  -0.698341  -0.698341  1.77157         0  Float64\n   7 │ Bland Chromatin               6.11192e-17  -0.998122  -0.181694  2.6758          0  Float64\n   8 │ Normal Nucleoli              -1.24677e-16  -0.612478  -0.612478  2.33576         0  Float64\n   9 │ Mitoses                       2.17818e-17  -0.348145  -0.348145  4.84614         0  Float64","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Good, now every column has a mean very close to zero, so the standardization was done correctly.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We now create our model with Elysivm","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"model = Elysivm.LSSVClassifier();\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"These are the values for the hyperparameter grid search. We need to find the best subset from this set of parameters. Although I will not do this here, the best approach is to find a set of good hyperparameters and then refine the search space around that set. That way we can ensure we will always get the best results.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"sigma_values = [0.5, 5.0, 10.0, 15.0, 25.0, 50.0, 100.0, 250.0, 500.0];\nr1 = MLJBase.range(model, :σ, values=sigma_values);\ngamma_values = [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0];\nr2 = MLJBase.range(model, :γ, values=gamma_values);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We now create a TunedModel that will use a 10-folds stratified cross validation scheme in order to find the best set of hyperparameters. The stratification is needed because the classes are somewhat imbalanced:","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Benign: 458 (65.5%)\nMalignant: 241 (34.5%)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"self_tuning_model = TunedModel(\n    model=model,\n    tuning=Grid(rng=rng),\n    resampling=StratifiedCV(nfolds=10),\n    range=[r1, r2],\n    measure=accuracy,\n    acceleration=CPUThreads(), # We use this to enable multithreading\n);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Once the best model is found, we create a machine with it, and fit it","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"mach = machine(self_tuning_model, X, y);\nfit!(mach, rows=train, verbosity=0);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We can now show the best hyperparameters found.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"fitted_params(mach).best_model","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"LSSVClassifier(\n    kernel = :rbf,\n    γ = 0.01,\n    σ = 0.5,\n    degree = 0)\u001b[34m @203\u001b[39m","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"And we test the trained model. We expect somewhere around 94%-96% accuracy.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"results = predict(mach, rows=test);\nacc = accuracy(results, y[test]);\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Show the accuracy for the testing set","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"println(acc * 100.0)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"94.73684210526316\n","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"As you can see, it is fairly easy to use Elysivm together with MLJ. We got a good accuracy result and this proves that the implementation is actually correct. This dataset is commonly used as a benchmark dataset to test new algorithms.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Types","page":"Reference","title":"Types","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [Elysivm]\nPages = [\"types.jl\"]\nPrivate = false","category":"page"},{"location":"reference/#Elysivm.LSSVC","page":"Reference","title":"Elysivm.LSSVC","text":"LSSVC <: SVM\nLSSVC(; kernel=:rbf, γ=1.0, σ=1.0, degree=0)\n\nThe type to hold a Least Squares Support Vector Classifier.\n\nFields\n\nkernel::Symbol: The kind of kernel to use for the non-linear mapping of the data. Can be one of the following: :rbf, :linear, or :poly.\nγ::Float64: The gamma hyperparameter that is intrinsic of the Least Squares version of the Support Vector Machines.\nσ::Float64: The hyperparameter for the RBF kernel.\ndegree::Int: The degree of the polynomial kernel. Only used if kernel is :poly.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Elysivm.LSSVR","page":"Reference","title":"Elysivm.LSSVR","text":"LSSVR <: SVM\nLSSVR(; kernel=:rbf, γ=1.0, σ=1.0, degree=0)\n\nThe type to hold a Least Squares Support Vector Regressor.\n\nFields\n\nkernel::Symbol: The kind of kernel to use for the non-linear mapping of the data. Can be one of the following: :rbf, :linear, or :poly.\nγ::Float64: The gamma hyperparameter that is intrinsic of the Least Squares version of the Support Vector Machines.\nσ::Float64: The hyperparameter for the RBF kernel.\ndegree::Int: The degree of the polynomial kernel. Only used if kernel is :poly.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Elysivm.SVM","page":"Reference","title":"Elysivm.SVM","text":"SVM\n\nA super type for both classifiers and regressors that are implemented as Support Vector Machines.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Methods","page":"Reference","title":"Methods","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [Elysivm]\nPages = [\"training.jl\"]\nPrivate = false","category":"page"},{"location":"reference/#Elysivm.svmpredict-Tuple{LSSVC,Any,AbstractArray{T,2} where T}","page":"Reference","title":"Elysivm.svmpredict","text":"svmpredict(svm::LSSVC, fits, xnew::AbstractMatrix) -> AbstractArray\n\nUses the information obtained from svmtrain such as the bias and weights to construct a decision function and predict new class values. For the classification problem only.\n\nArguments\n\nsvm::LSSVC: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nfits: It can be any container data structure but it must have four elements: x, the data matrix; y, the labels vector; α, the weights; and b, the bias.\nxnew::AbstractMatrix: The data matrix that contains the new instances to be predicted.\n\nReturns\n\nArray: The labels corresponding to the prediction to each of the instances in xnew.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Elysivm.svmpredict-Tuple{LSSVR,Any,AbstractArray{T,2} where T}","page":"Reference","title":"Elysivm.svmpredict","text":"svmpredict(svm::LSSVR, fits, xnew::AbstractMatrix) -> AbstractArray\n\nUses the information obtained from svmtrain such as the bias and weights to construct a decision function and predict the new values of the function. For the regression problem only.\n\nArguments\n\nsvm::LSSVR: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nfits: It can be any container data structure but it must have four elements: x, the data matrix; y, the labels vector; α, the weights; and b, the bias.\nxnew::AbstractMatrix: The data matrix that contains the new instances to be predicted.\n\nReturns\n\nArray: The labels corresponding to the prediction to each of the instances in xnew.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Elysivm.svmtrain-Tuple{LSSVC,AbstractArray{T,2} where T,AbstractArray{T,1} where T}","page":"Reference","title":"Elysivm.svmtrain","text":"svmtrain(svm::LSSVC, x::AbstractMatrix, y::AbstractVector) -> Tuple\n\nSolves a Least Squares Support Vector classification problem using the Conjugate Gradient method. In particular, it uses the Lanczos process due to the fact that the matrices are symmetric.\n\nArguments\n\nsvm::LSSVC: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nx::AbstractMatrix: The data matrix with the features. It is expected that this array is already standardized, i.e. the mean for each feature is zero and its standard deviation is one.\ny::AbstractVector: A vector that contains the classes. It is expected that there are only two classes, -1 and 1.\n\nReturns\n\nTuple: A tuple containing x, y and the following two elements:\nb: Contains the bias for the decision function.\nα: Contains the weights for the decision function.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Elysivm.svmtrain-Tuple{LSSVR,AbstractArray{T,2} where T,AbstractArray{T,1} where T}","page":"Reference","title":"Elysivm.svmtrain","text":"svmtrain(svm::LSSVR, x::AbstractMatrix, y::AbstractVector) -> Tuple\n\nSolves a Least Squares Support Vector regression problem using the Conjugate Gradient method. In particular, it uses the Lanczos process due to the fact that the matrices are symmetric.\n\nArguments\n\nsvm::LSSVR: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nx::AbstractMatrix: The data matrix with the features. It is expected that this array is already standardized, i.e. the mean for each feature is zero and its standard deviation is one.\ny::AbstractVector: A vector that contains the continuous value of the function estimation. The elements can be any subtype of Real.\n\nReturns\n\nTuple: A tuple containing x and the following two elements:\nb: Contains the bias for the decision function.\nα: Contains the weights for the decision function.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Elysivm","category":"page"},{"location":"#Elysivm","page":"Home","title":"Elysivm","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is Elysivm, a Least Squares Support Vector Machine (LSSVM) implementation in pure Julia. It is meant to be used together with the fantastic MLJ.jl Machine Learning framework.","category":"page"},{"location":"#Formulation","page":"Home","title":"Formulation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"It is a re-formulation of the classical Support Vector Machine (SVM) formalism. In this case we attempt to solve a least squares problem which is faster[1], instead of the classic quadratic, convex optimization problem that is solved in the original Support Vector Machine.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the case of Elysivm we use the conjugate gradient method, in particular the Lanczos version[2] due to the fact that we solve several linear systems which have the the following structure","category":"page"},{"location":"","page":"Home","title":"Home","text":"A mathbfx = mathbfb","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the matrix A is symmetric.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This fact makes it a great candidate for the Lanczos algorithm, a very fast, iterative procedure based on Krylov subspace methods. The implementation used here is that from the Krylov.jl package.","category":"page"},{"location":"#Rationale","page":"Home","title":"Rationale","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SVM has been the most known and used formulation, but here are some pros and cons for using LSSVMs and Elysivm.","category":"page"},{"location":"#Advantages","page":"Home","title":"Advantages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The LSSVM is a great alternative to the classic SVM in the following things:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Solving a linear system is much easier and faster than solving a quadratic optimization problem.\nSome useful properties from numerical linear algebra can be exploited in order to solve the new optimization problem.\nOne can potentialy train of thousands or millions of instances using LSSVM, something that the classic SVM cannot do. This is possible using the fixed size LSSVM[3].\nLess hyperparemeters to tune. LSSVM only has one intrinsic hyperparamter, whereas the SVM has at least two. This is without taking into account the kernel's hyperparameter.","category":"page"},{"location":"#Disadvantages","page":"Home","title":"Disadvantages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"But there are some important shortcommings for the LSSVM, namely:","category":"page"},{"location":"","page":"Home","title":"Home","text":"In contrast with the classic SVM, the decision function lack all sparseness. Every single dataset instance must be used to train the LSSVM. This can become troublesome for very large problems because all the instances must fit into memory.\nThere is complete lack of interpretation for the support vectors, which are the data instances that are used to construct the decision function. Because all data instances are used, every instance is effectively a support vector and this removes any interpretation for the importance of each instance on the model's performance.","category":"page"},{"location":"#Bibliography","page":"Home","title":"Bibliography","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[1]: Suykens, J. A., & Vandewalle, J. (1999). Least squares support vector machine classifiers. Neural processing letters, 9(3), 293-300.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[2]: Fasano, G. (2007). Lanczos conjugate-gradient method and pseudoinverse computation on indefinite and singular systems. Journal of optimization theory and applications, 132(2), 267-285.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[3]: Espinoza, M., Suykens, J. A., & De Moor, B. (2006). Fixed-size least squares support vector machines: A large scale application in electrical load forecasting. Computational Management Science, 3(2), 113-129.","category":"page"}]
}
